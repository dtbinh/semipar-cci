#TensorDecomp Based Classification on BNU3 dataset

In this experiment, we carry out tensor decomposition on one dataset and obtain low dimensional representation of 46 subjects. The goal is to assess if TensorDecomp results in any loss (if not improvement) in discriminability wrt labels (gender in this case).


## Tensor decomp

Tensor decomp first, without using any info about labels.

```{r}
require("TensorEmbedding")
require("base")
require("plyr")
setwd("~/git/semipar-cci/")

#load processed data
load("Data/processed/BNU3.RDa")

BNU3<-dataList

m<- length(BNU3)
n<- nrow(BNU3[[1]]$A)

A<- array(0,dim = c(n,n,m))
for(i in 1:m){
  A[,,i]<- BNU3[[i]]$A
}


id<- unlist(lapply(BNU3, function(x){x$id}))

k<- 30

#tensor decomp:
# fit <- TensorEmbedding::symm_group_tensor_decomp(A, id, n, m, k, 1000, 1E-5, 1E-5)
# save(fit,file="usecase/BNU_fit.Rda")

#let's load the serialized one for now
load(file="usecase/BNU_fit.Rda")
```


Plot the eigenvalues, colored by labels
```{r}

SEX<- unlist(lapply(BNU3, function(x){x$SEX}))

plot(c(1,k),range(fit$diagC),type="n")
for(i in 1:m){
  lines(fit$diagC[i,],col=SEX[i])
}
```

## Classification

###QDA
To control variance in the covariances, we use simple scalar*identiy for the two classes

1. On TensorDecomp-produced vector

```{r}

#require("ggplot2")
# overlayHist<- function(x,y){
#   dat<- data.frame(
#     "class"=y,
#     "feature"=x
#   )
#   ggplot(dat, aes(x=feature, fill=as.factor(class))) + geom_histogram(alpha=0.2, position="identity", bins = 20)
#   
# }
# 
# overlayHist(fit$diagC[,30],y)

y<- SEX-1

computeProb<-function(x,m1,cov1,m0,cov0){
  d1<-(x-m1)
  log_prob1<- -(d1%*%solve(cov1,d1) + log(det(cov1)))/2
  
  d0<-(x-m0)
  log_prob0<- -(d0%*%solve(cov0,d0) + log(det(cov0)))/2
  
  prob<-  1/(1+exp(log_prob0-log_prob1))
  prob 
}


# sel<- order(apply(fit$diagC,MARGIN = 2,sd) / colMeans(fit$diagC))< 20

useTopKforC14 <-function(k){
  looCV <- function(i,sel){
    # x<- fit$diagC[y==1,]
    x<- fit$diagC[-i,sel][y[-i]==1,]
    m1<- colMeans(x)
    # cov1<- diag(apply(x, 2, sd)^2)
    cov1<- diag( mean((t(x)- m1)^2),length(m1))
    
    
    # diag(cov1)<- diag(cov1)+ 1E-8
    
    # x<- fit$diagC[y==0,]
    x<- fit$diagC[-i,sel][y[-i]==0,]
    m0<- colMeans(x)
    # cov0<- diag(apply(x, 2, sd)^2)
    # cov0<- cov(x)
    # diag(cov0)<- diag(cov0)+ 1E-8
    cov0<- diag( mean((t(x)- m0)^2),length(m0))

    
    
    pred<- computeProb(fit$diagC[i,sel],m1,cov1,m0,cov0)
    
    1*(as.numeric(pred)>0.5)== y[i]
  }
  
  sel<- 1:k
  estCorrect<- unlist(lapply(1:m, function(x){looCV(x,sel)}))
  1-sum(estCorrect)/m
}

```

With different lower dimension K

```{R}
KError<- sapply(2:k, useTopKforC14)
plot(2:k,KError,col="red",type="l",xlab="k",ylab="C14 Error Rate")

min(KError)

optK<- c(2:k)[KError==min(KError)]
```


2. On adjecency matrices
```{R}

lowerTri<-lower.tri(BNU3[[1]]$A)

matA<- matrix(0,m,n*(n-1)/2)
for(i in 1:m){
  matA[i,]<- BNU3[[i]]$A[lowerTri]
}


naiveEstimator<-function(exc_i){

  data1 <- matA[-exc_i,][y[-exc_i] ==1,]
  data0 <- matA[-exc_i,][y[-exc_i] ==0,]
 

  M1<- colMeans(data1)
  M0<- colMeans(data0)

  # C1<- sd(data1)^2
  # C0<- sd(data0)^2
  C1<- mean((t(data1)- M1)^2)
  C0<- mean((t(data0)- M0)^2)

  
 
 p1<- -sum((matA[exc_i,]-M1)^2/C1)-n*n*log(C1)
 p2<- -sum((matA[exc_i,]-M0)^2/C0)-n*n*log(C0)
 
 1*(p1>p2) == (SEX[exc_i]-1)
 
}

NEestCorrect<- unlist(lapply(1:m, naiveEstimator))

1-sum(NEestCorrect)/m

```

###K-nearest neighbor


KNN function:

```{R}

knn<- function(i,dist,k=10,cl){
  o<- order(dist[i,])
  ct<-count(cl[o>1 & o<=(k+1)])
  majority_vote<- ct$x[ct$freq == max(ct$freq)] 
  if(length(majority_vote)>1)
    sample(x = majority_vote,size = 1)
  else
    majority_vote
}

knn_mc_rate <-function(dist,k){
sum(SEX != sapply(c(1:m), function(x){knn(x,dist,k,SEX)}))/m
}


```



1. On TensorDecomp-produced vector

```{R}


dist<- matrix(0,m,m)
for(i in 1:m){
  for(j in 1:(i)){
    dist[i,j]<- sqrt(sum((fit$diagC[i,]-fit$diagC[j,])^2))
    dist[j,i]<- dist[i,j]
  }
}


knn_rate_vs_k<- sapply(c(1:30), function(k)knn_mc_rate(dist,k))

plot(c(1:30),knn_rate_vs_k, type="l",xlab="k",ylab="C14 Error Rate", col="red")

min(knn_rate_vs_k)

```


2. On full adjacency matrices

```{R}


dist<- matrix(0,m,m)
for(i in 1:m){
  for(j in 1:(i)){
    dist[i,j]<- sqrt(sum((matA[i,]-matA[j,])^2))
    dist[j,i]<- dist[i,j]
  }
}


knn_rate_vs_k<- sapply(c(1:(30)), function(k)knn_mc_rate(dist,k))

plot(c(1:(30)),knn_rate_vs_k, type="l",xlab="k",ylab="C14 Error Rate", col="red")

min(knn_rate_vs_k)

```

##Conclusion

The TensorDecomp-produced vectors seem to perform slightly better than the raw adjecency matrices. This suggests that TensorDecomp preserves most of the useful variabiltiy w.r.t labels, in one dataset.
